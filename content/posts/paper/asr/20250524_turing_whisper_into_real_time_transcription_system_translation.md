---
title: Turning Whisper into Real-Time Transcription System 번역
subtitle: ""
draft: false
date: 2025-05-24 21:05:00 +0900
categories: [Paper, Translation]
tags: [Paper, Machine Learning, ASR, Whisper]
math: true
mermaid: true
image:
  path: /images/paper/asr/turning_whisper_into_real_time_transcription_system/Subject.png
---


{{< figure src="/images/paper/asr/turning_whisper_into_real_time_transcription_system/Subject.png" alt="subject" class="center" width="80%" >}}

## Abstract

Whisper은 다국어 음성 인식과 번역 모델에서 최신 SOTA 모델 중 하나이다.
하지만, 실시간 전사를 위해 설계된 것은 아니다.
본 논문에서, 우리는 Whisper을 기반으로 하여 Whisper-Streaming을 개발한다.
이것은 Whisper와 유사한 모델들에서 실시간 음성 전사 및 번역을 구연한다.
Whisper-Streaming은 self-adaptive latency를 사용하는 local agreement 정책을 실시간 인식을 위해 사용한다.
우리는 Whisper-Streaming이 분할되어 있지 않은 긴 형태의 음성 인식 테스트 셋에서 높은 퀄리티와 3.3 초의 지연시간을 보이며, 다국어 회의의 실시간 전사 서비스의 한 요소로 견고함과 실용성을 입증하였다.

## 1. Introduction

Whisper은 97개의 언어에서의 자동 음성 인식(ASR)과 96개의 언어에서 영어로의 번역을 하는 최신 SOTA이다.
Whisper 모델은 MIT license로 공개되어있다.
하지만, 현재 공개된 Whisper의 추론 구현은 보통 처리 시점에서 어떠한 시간 제약도 없이 완전히 확보된 음성 데이터만을 오프라인 처리를 허용한다.

실시간 스트리밍 모드는 실시간 자막과 같은 특정 상황에서 유용하다.
이는 입력되는 음성 오디오가 녹음되는 시간에 연산 되어야함을 의미한다.
전사 및 번역은 2초 정도의 짧은 지연 시간 동안 전달되어야한다.
Whisper을 사용한 Streaming 구현도 몇몇 존재하지만 그것들의 접근 방식은 다소 단순하다.
그들은 예를들어 30초씩 오디오를 분할하고 연산한다.
그 기법들의 지연시간은 크며, 단순하게 내용을 보지 않고 분할하기에 단어가 중간에서 잘리는 경우가 있어 구간 경계의 품질또안 낮다.

이 연구에서, 우리는 간단하고 효율적인 LocalAgreement 알고리즘을 사용하여 Whisper를 동시 스트리밍 모드에서 구현, 평가 및 시연한다.
LocalAgreement 알고리즘은 모든 전체 시퀀스 입출력 모델을 동시 스트리밍 모드에서 작동하도록 변환할 수 있는 스트리밍 정책 중 하나이다.
이 알고리즘은 IWSLT 2022 동시 음성 번역 공유 과제에서 우승한 시스템인 CUNI-KIT에서 사용되었다.
우리의 구현은 Whisper-Streaming이라고 부른다. 하지만, Whisper과 유사한 다른 모델에도 적용가능하다.
우리의 평가 결과에 따르면, Whisper-Streaming은 유럽 의회 연설 테스트 셋 ESIC의 영어 ASR 부문에서 3.3초의 평균 지연시간을 달성했다.
이는 고성능 처리 장치 NVIDIA A40 GPU에서 구동한 결과이다.
또한 독일어와 체코어에 대해서 ASR을 테스트 했으며, 결과와 최적 파라미터에 대한 제안도 함께 제시한다.

본 연구의 기여는 Whisper-Streaming의 구현과 평가 및 시연이다.
주어진 Whisper-Streaming은 빠르고 쉽게 제품화 될 수 있다.
우리는 동시 처리 모드용 알고리즘과 같은 최신 과학적 결과가 산업 연구원과 개발자에게 접근 가능하고 실제로 활용될 수 있도록 하는 것이 목표이다.
나아가, 우리는 Whisper-Streaming의 성능에 대한 실뢰성 있는 평가와 연구 커뮤니티에 그 결과를 공유함으로써, 실생활에서 활용 가능한 실시간 전사 솔루션의 연구 및 개발을 더욱 촉진하고자 한다.
우리는 본 연구 결과가 향후 비교 연구를 위한 강력한 기준선으로 활용될 수 있을 것으로 기대한다.

우리는 Whisper-Streaming을 시연 영상과 함께 공개한다.

## 2. Background

이 절에서는 본 연구의 백엔드 구성 요소에 대한 배경을 설명한다.

Whisper은 막대한 양의 다국어 데이터로 훈련된 stt 전사와 번역을 위한 Transformer 모델이다.
우리는 Whisper 모델 크기 옵션에서 가장 높은 퀄리티를 달성한 Large-v2를 사용한다.
whisper backend의 원본은 다소 느리기 때문에 우리는 Transformer 모델용 고속 추론 엔진인 CTranslate2를 활용하여 Whisper 추론을 재구현한 faster-whisper를 사용한다.
이는 표준 구현보다 약 4배 빠르다.
우리는 16-bit float 정밀도를 사용한다.

우리는 주로 Whisper을 사용하지만, 구현된 시스템의 기반 모델은 단어 수준의 타임스탬프와 구두점을 생성할 수 있는 경우, MMS와 같은 다른 음성-텍스트 전사 또는 번역 모델로도 쉽게 대체할 수 있다.

**Streaming**
문장간 일관성을 위해 사용할 수 있는 이전 출력 시퀀스 $ s $가 주어질 때, 입력 시퀀스 $ c_1, \cdots, c_n $을 출력 시퀀스 $ t_1, \cdots, t_m $으로 연산하는 모델 $ M $을 가정하자.
스트리밍은 입력 시퀀스를 한 번에 하나의 청크씩 순차적으로 받아들이면서 동시에 출력을 생성하는 방식이다.
스트리밍 정책 $ P $는 시간 $ T $에서의 결과 세그먼트 $ t_T $를 다음과 같이 예측한다.
$ t_T := P_M(c_i \gt T |s, t_j \gt T) $
이것은 model $ M $을 입력 청크 $ c_i \gt T $, 이전 타겟 시퀀스 $ s $, 그리고 이전까지의 출력 세그뭔트 $ t_j \gt T $에 기반하여 동작시킨다.

이 정책은 새로운 입력 세그먼트가 사용가능한 매 시간 동작한다.
문맥을 기다리는 경우에 출력 세그먼트가 비어있을 수 있다.
이 정책은 지역을 최소화하고 출력 품질을 극대화하는것을 목표로 한다.

스트리밍은 본래 동시 통역을 위해 제안되었다.
그러나 이 개념은 ASR을 포함하여 모든 Sequence-to-Sequence 작업에 적용 가능하다.

**LocalAgreement**
[LocalAgreement](https://www.isca-archive.org/interspeech_2020/liu20s_interspeech.pdf)는 스트리밍 정책 중 하나로, n개의 연속된 입력 청크에서의 모델 출력값의 가장 긴 공통 접두사를 출력하거나, 사용가능한 청크 수가 n 보다 적을 경우 빈 세그먼트를 출력한다.
IWSLT 2022 동시 번역 공유 과제에서 CUNI-KIT 시스템은 LocalAgreement와 다른 정책 (hold-n & wait-k)과 다른 청크 사이즈에서 비교했다.
그 결과 n=2인 LocalAgreement가 가장 효과적인 정책임을 발견했다.
따라서, 우리는 안정화된 출력 세그먼트를 식별하기위해 LocalAgreement2를 사용한다.

## 3. Whisper-Streaming

우리는 Whisper-Streaming의 핵습 구성 요소와 내부 작동 방식을 설명한다
Whisper-Streaming은 업데이트 루프, 오디오 버퍼, 오디오 버퍼에서 확정된 출력 생략, 버퍼 자르기, 문장 간 문맥 연결을 위한 결합 처리, 선택적 음성 활동 감지로 구성된다.

**Update Loop**
Whisper-Streaming의 핵심은 입력 오디오 청크를 받아들이고 스트리밍 정책을 갱신하는 루프를 활용하는 프로그램이다.
파라미터 MinChunkSize는 지연시간과 퀄리티를 제어하고, 반복마다 처리할 최소 길이를 결정한다.
만약 업데이터 계산이 MinChunkSize를 초과하면, 누적 오디오 입력에 대해 즉시 다음 업데이트를 수행한다.
이 파라미터는 지연시간과 품질 모두에 영향을 미친다.

{{< figure src="/images/paper/asr/turning_whisper_into_real_time_transcription_system/Figure1.png" alt="Figure 1" title="Figure 1" class="center" width="80%" caption="세번의 연속된 업데이트 처리 과정에 대한 예시. 노란색 하이라이트된 텍스트는 'prompt'이며, 이전 문맥을 나타낸다. 검정색 테두리의 사각형은 오디오 버퍼이며, 내부 텍스트는 해당 음성 구간에 대한 Whisper의 전사 결과이다. 파랑색의 수직 선은 버퍼를 두개의 파트로 나누기 위한 타임스탬프이며, 왼쪽은 이전에 확정된 것을, 오른쪽은 확정되지 않은 것이다. LocalAgreement-2 정책, 즉 최장 공통 접두사를 찾는 것은 확정되지 않은 오른쪽 파트의 두번의 다음 업데이트에 걸쳐 적용된다. 가장 긴 공통의 접두사는 초록색으로 하이라이트되어 있으며, 초록색의 밑줄은 새로운 확정된 출력에 하이라이트 되어있고, 초록색 대시 밑줄은 이전 및 이후에 확정된 출력이다. 회색 밑줄은 확정부분에서 무시된 업데이트를 시연한다.">}}

**Audio buffer**
Whisper는 전체 시퀀스를 담고있는 30초 이상의 긴 시퀀스를 다루기위해 학습되었다.
Whisper은 구두점과 단어레벨 타임스탬프를 제공한다.
해당 과정은 Figure 1에 설명되어 있다.
각 업데이트는 들어오는 오디오를 오디오 버퍼 위에 저장하고 Whisper로 버퍼 전체를 연산하는 과정을 포함한다.
우리는 Whisper의 높은 품질을 유지하기 위해 버퍼는 항상 새로운 문장으로 시작되도록 유지하는 것이 불변 조건이다.
현재와 이전 Whisper 결과에 대해 LocalAgreement-2가 적용된다.
"확정된 출력"의 마지막 단어의 타임스탬프가 저장된다.
이후 업데이트에서, 항상 버퍼의 시작 부터 whisper 처리를 다시 수행한다.
이때 "확정된 출력" 이전 구간도 함께 처리 대상에 포함된다(Figure 1에서 회색 배경으로 표시된다).
확정된 출력 구간 내 전사 내용에 변경이 생기더라도, 의미에 큰 영향을 주지 않는 한 이러한 변경은 무시된다.

**Skipping the confirmed part**
이전 업데이트에서 마지막으로 확정된 단어와 관련된 전사된 단어의 위치를 결정할때, 새로운 오디오 청크 때문에 잠재적 부정확함을 고려하여 whisper의 타임스탬프를 업데이트해야 한다.
만약 단어의 타임스탬프가 이전 확정 단어의 타임스탬프로부터 1초 이내에 위치한다면 우리는 앞의 n-grams(1~5 사이)을 이전 확장 출력의 접미사와 비교한다.
만약 이들이 일치할 경우, 해당 단어를 스킵한다.
하지만, 이 규칙은 향후 연구에서 다음과 같은 방식으로 더욱 정교화될 수 있다.
문자 편집 거리 임계값 설정 및 조정과 n-그램에서 구두점과 대소문자 제거 등의 기법을 통해서 유연성을 높일 수 있다.

**Trimming the audio buffer**
지연시간이 과도하게 길어지는 것을 피하기 위해, 오디오 버퍼는 약 30초로 제한된다.
확정된 출력이 문장 끝의 구두점과 새로운 문장의 시작 단어를 가질 때, 버퍼를 구두점의 타임스탬프에서부터 잘라낸다.
언어 특정 [**문장 분할 도구**](https://aclanthology.org/P07-2045.pdf)가 이번 제안에 사용된다. 버퍼가 항상 하나의 문장을 포함하도록 한다.
그럼에도 불구하고 버퍼의 길이가 30 초를 초과하면, 우리는 Whisper가 마지막으로 확정된 출력으로 표시한 구간까지만 버퍼에 유지한다.

**Joining for inter-sentence context**
Whisper 전사 함수는 데이터에서의 일관성(문체, 용어, 문장 간 참조의 일관성)을 유지하기 위해 "prompt" 파라미터를 사용한다.
우리는 Figure 1에서 보여준것 처럼(노랑색 배경 문자) 이전 오디오 버퍼에서 확정된 출력에서 마지막 200단어들을 추출하여 "prompt" 파라미터로 사용한다.

**Voice activity detection**
Whisper에는 기본 음성 활동 감지(VAD) 필터를 활성화 하거나 비활성화 할 수 있는 파라미터가 있다. 이 설정은 전사의 품질과 지연 시간 모두에 영향을 미친다.

## 4. Benchmarking Settings

우리는 우리의 모델을 평가하기위해 사용한 데이터셋, 평가 지표, 설정과 하드웨어에 대해 설명한다.

**Evaluation Data**
지연 시관과 품질 분석을 위해, 우리는 179개의 문서를 포함한 영어, 독일어, 체코어 ASR 평가에 사용되는 수동 전사된 [**ESIC 코퍼스**](https://www.isca-archive.org/interspeech_2021/machacek21_interspeech.pdf)의 개발 세트를 활용한다.
이 데이터셋은 유럽 의회에서의 5시간의 원어 영어 연설과 독일어와 체코어의 동시 통역을 포함한다.
또한, 수동 전사본과 단어 단위 타임스탬프과 함께 오디오 트랙을 제공한다.

**WER**
우리는 ASR 품질의 표준 측정으로 구두점과 대소문자가 제거된 WER을 사용한다.

**Latency**
우리의 지연시간 분석에서, 우리는 자체적인 방법을 구현했다. 이 방법은 ESIC 코퍼스에 포함된 타임스탬프를 활용하여, 정답 전사를 ASR 출력과 edit distance(Levenshtein distance)를 이용해 정렬하는 방식이다.
이를 통해 각 정답 단어에 대한 edit operations를 결정할 수 있다.
ASR이 출력한 단어와 일치하는 정답 단어가 발화된 시점의 시간 차이를 측정하여 ASR 지연 시간을 계산하였으며, ASR에 의해 삭제된 단어는 제외한다.
우리는 각 문서단위로 평균 지연 시간을 계산했으며, 다른 문서간 설정을 비교할 때, 우리는 평균 지연시간과 표준 편차도 함께 보고한다.

**Hardware**
벤치마크를 위해, 우리는 NVIDIA A40 GPU를 사용한다.
우리는 whisper을 클러스터 컴퓨터 내에서 실행하며, 이것은 다른 프로세스와 같은 시간에 사용된다.
따라서, 자원 경쟁이 발생하고 지연 시간에 영향을 줄 수 있다.
주어진 서비스를 위해 전용 서버를 항상 확보하는 것은 불가능하기 때문에, 이것은 우리의 평가가 매우 현실적으로 만든다.
지연 시간 지표에서 변동이 발생할 수 있으므로, 평균과 표준 편차를 함께 보고한다.

**Ensuring Reproducibility**
우리는 장문 전사의 실시간 연산에 대해 시뮬레이션하고, Whisper가 결과를 출력한 시점을 기록한다.
우리는 전체적으로 우리가 제어할 수 없는 하나의 클러스터내의 컴퓨터로 시뮬레이션을 동작한다.
우리의 시뮬레이션 과정을 위해, 하나의 GPU와 충분한 CPU들과 RAM 용량을 햘당받는다.
하지만, 다른 프로세스들이 같은 시간에 동작하는 일이 발생할 수 있다.
이것은 CPU와 RAM 사용이 예측 불가능하게 우리의 시뮬레이션을 느리게만들 수 있다.
만약 MinChunkSize가 한번의 업데이트 처리 시간보다 적을 경우, 동일 시뮬레이션을 두 번 실행하더라도 청크로 나누는 방식이 달라져 WER과 지연 시간 겨로가가 달라질 수 있다.

따라서, 우리는 퀄리티와 지연 시간의 표준 편차를 측정하기 위해 한 문서를 같은 설정에서 10번씩 시뮬레이션 한다.
설정은 ESIC dev.20080925.013_007 문서의 영어 전사를 사용하며, 이것은 3분 36초 길이이다. NVIDIA A40 또는 L40 GPU와 48GB GPU RAM, 8 blocked CPU 코어들과 200GB의 CPU RAM, VAD 필터 여부, MinChunkSize는 0.1초로 하였다.

{{< figure src="/images/paper/asr/turning_whisper_into_real_time_transcription_system/Table1.png" alt="Table 1" title="Table 1" class="center" width="50%" caption="영어 ASR에 대해 ESIC dev.20080925.013_007 문서를 대상으로 MinChunkSize를 0.1초로 설정하여 10회 반복 실행한 평균(±표준편차) WER 및 지연 시간 결과이다. 실험은 VAD 필터 사용 여부와 두 가지 GPU 종류에 따라 나뉘며, 굵게 표시된 항목은 이후 실험에서 사용된 최종 설정이다.">}}

결과는 Table 1에 제시되어 있다. WER에서 작고 무시할 수 있을 정도의 1% 이하 또는 그에 가까운 표준 편차가 관찰되었다.
평균 지연시간에서의 표준 편차는 훨신 크며, 설정에 따라 0.36에서 0.81초까지 나타났다.
우리는 제어할 수 없는 컴퓨터 조건 때문에 지연시간의 표준 편차를 의식하고 고려해야한다.

## 5. Results

우리는 영어와 독일어, 체코어 ASR에서의 다양한 설정에서 Whisper-Streaming을 평가했다.
우리는 먼저 최적 설정을 결정하기 위해 이상치와 VAD 여부에 따른 영향을 보여준다.
그리고 이 설정들에 대한 우리의 결과를 제시한다.

**Outlier**
여러 설정에서 처리한 결과, dev2.20101213.015_018_EN_Gallagher 문서의 영어 ASR에서 이례적으로 높은 WER이 관찰되었다.
우리는 ESIC 데이터 셋 내의 노이즈 때문임을 확인했다.
해당 문서의 전반부는 알려진것과 다르게 영어가 아닌 아일랜드어로 되어 있었다.
정답 전사는 오직 영어 부분만 있었으며, Whisper는 아일랜드어 까지 모두 전사했으며 레퍼런스보다 더 정밀한 전사를 이끌었다.
Gallagher 문서를 제외하면, 모든 보고된 설정들은 WER 0%부터 52%사이와 평균 지연율 0에서 16.1초를 달성했다.

{{< figure src="/images/paper/asr/turning_whisper_into_real_time_transcription_system/Table2.png" alt="Table 2" title="Table 2" class="center" width="50%" caption="영어 ASR에 대해 ESIC dev.20080925.013_007 문서를 대상으로 MinChunkSize를 0.1초로 설정하여 10회 반복 실행한 평균(±표준편차) WER 및 지연 시간 결과이다. 실험은 VAD 필터 사용 여부와 두 가지 GPU 종류에 따라 나뉘며, 굵게 표시된 항목은 이후 실험에서 사용된 최종 설정이다.어 ASR에 대해 ESIC dev.20080925.013_007 문서를 대상으로 MinChunkSize를 0.1초로 설정하여 10회 반복 실행한 평균(±표준편차) WER 및 지연 시간 결과이다. 실험은 VAD 필터 사용 여부와 두 가지 GPU 종류에 따라 나뉘며, 굵게 표시된 항목은 이후 실험에서 사용된 최종 설정이다.">}}

{{< figure src="/images/paper/asr/turning_whisper_into_real_time_transcription_system/Figure2.png" alt="Figure 2" title="Figure 2" class="center" width="50%" caption="시간 지연율과 품질에서의 VAD 필터 영향. VAD 활성화와 비활성화에서 영어와 독일어의 뚜렷한 차이는 독일어가 동시 통역사의 발화이기 때문이다.">}}

**VAD**
우리는 Whisper 백엔드에 포함된 VAD 필터의 효과에 대해 연구했다.
결과는 Table 2와 Figure 2에 제시되어 있다.
실험을 통해, ESIC 코퍼스의 영어 원어 연설에서는 VAD 필터를 비활성화 하는 것이 바람직하다는 사실을 확인하였다. 이는 해당 연설이 매우 유창하게 이어지며, 침묵이나 비음성 음향이 거의 없기 때문이다.
VAD 없이, 퀄리티는 거의 동일했으며 (차이는 0.2% WER 이내), 평균 지연 시간은 현저히 낮아져, 0.23초에서 0.41초 사이를 기록했다.

동시 통역 연산을 위해서, 우리는 VAD 필터를 활성화하길 권장한다.
동시 통역사의 발화에는 많은 정지들이 포함된다.
특히 문맥을 기다릴 때 기다린다.
VAD를 사용하면, 지연 시간이 오직 0.1 초 커진다.
왜냐하면 VAD는 자주 침묵을 필터링해주어 연산 부하를 감소시키기 때문이다.
독일어에서 MinChunkSize를 짧게 설정하면 2%에서 3% WER이 감소하며, VAD를 사용했을때 품질이 상당히 높아졌다.
큰 청크사이즈에서, 품질은 거의 같았다(MinChunkSize 2초일때, 0.3% WER 차이).
이는 큰 청크 사이즈는 모델에 많은 문맥 정보가 제공되기 때문에 불확실한 출력을 생성할 가능성이 줄어들기 때문이다.
그러므로, 우리는 독일어와 체코어의 동시 통역에서 VAD를 활성화했고, 우리는 영어 원어 연설에서는 비활성화 했다.

실제 환경의 설정을 위해서, 우리는 발화가 시작하기 전에 Whisper-Streaming을 시작하는 것을 권장한다.
이는 첫번째 단어를 놓치지 않기 위함이며, VAD 필터 또한 침묵과 비음성 소리로 인해 Whisper의 잘못된 추론을 방지하기 위해 켜는 것을 추천한다.
만약 지연시간 감소가 중요하다면, VAD 세팅의 적응형 프로토콜을 구현할 수 있다.

{{< figure src="/images/paper/asr/turning_whisper_into_real_time_transcription_system/Figure3.png" alt="Figure 3" title="Figure 3" class="center" width="50%" caption="GPU bound 연산을 고려한것과 고려하지 않는 것의 지연시간과 품질(직선과 점 vs 점선과 곱표), 그리고 오프라인 WER을 표시했다(별표와 밝은 수직 선). VAD는 영어에서 비활성화 했고, 다른 독일어와 체코어에서 활성화 했다.">}}

{{< figure src="/images/paper/asr/turning_whisper_into_real_time_transcription_system/Table3.png" alt="Table 3" title="Table 3" class="center" width="50%" caption="서로 다른 MinChunkSize('m.ch.')를 사용한 세개의 언어 트랙에서의 ESIC 개발 셋의 Whisper-Streaming의 WER과 평균 지연시간. 현실적인 설정은 computationally aware('aw.')이며, 오프라인 WER('offline'), 계산 지연을 고려하지 않는 시뮬레이션('un.') 결과가 함께 비교되었다. 데이터는 Figure 3와 같다.">}}

**Performance**
Table 3와 Figure 3는 ESIC 검증 셋에서 Whisper-Streaming의 세 언어에 대한 WER과 평균지연 시간을 요약한 것이다.
전체적으로, MinChunkSize가 1초일 때, 평균 지연시간이 영어에서 3.3초, 독일어에서 4.4초, 체코어에서 4.8초이며, WER은 오프라인 모드일때보다 영어와 독일어에서는 2% 높고, 체코어에서는 6% 높다.
WER과 지연 시간 모두 영어에서 가장 낮았고, 그 다음 독일어 체코어 순서다.
이것은 Whisper 모델 학습에 사용된 언어별 데이터의 양과, 언어의 형태론적 복잡성과 관련이 있다.
지연 시간은 출력에 대한 불확실성이 클수록 증가하는데, 이는 확정된 결과에 도달하기 위해 더 많은 업데이트가 필요하기 떄문이다. 또한, MinChunkSize가 클수록 지연 시간도 증가하지만, 문맥이 더 많이 주어지므로 품질은 향상된다.

**Offline mode WER**
우리는 최고 성능 추정치의 성정과 결과를 비교했다.
그 중 하나는 오프라인 모드로, 처리 시간에 어떠한 한계도 없는 전체 오디오 문서가 녹음된 뒤 연산한다.
이것은 기본값이며 가장 최적화된 whisper의 설정이다.
VAD를 사용하는 오프라인 모드에서 WER은 문맥 크기가 제한되지 않아 스트리밍 모드보다 더 낮게 나타난다.
스트리밍 모드에서 사용하지 못하며 한계인 미래 문맥까지도 모델이 활용할 수 있기 때문이다.
또한, 오프라인 모드에서 장문 음성을 처리 청크로 나누는 내부 분할 방식도 최적화되어 있어 전사 품질이 더욱 향상된다.

**Computationally unaware latency**
다른 비교 설정은 계산 지연을 고려하지 않는 시뮬레이션이다.
Whisper의 어떠한 음성 구간 연산도 즉시 실행되는 비현실적인 가정을 사용한다.
그러므로, 연산에 의해 생긴 지연 시간은 지연시간 측정에 포함되지 않는다.
지연 시간은 언어의 불확실성에 의해 생긴 지연시간만 포함된다.
계산 지연을 고려하거나 고려하지 않는 평가의 차이는 하드웨어 또는 인터페이스 알고리즘을 최적화함으로서 감소시킬 수 있다.
계산을 고려하지 않는 시간 지연은 모델 또는 스트리밍 정책 향상을 통해 감소될 수 있다.

우리는 고려되지 않은 평균 계산 지연 시간이 청크 사이즈의 약 2배로 관찰됐다.
이것은 두개의 연속적인 업데이트의 LocalAgreement를 사용하기 때문에 예상했던 바이다.
그러나, 영어의 연산 과정은 실제로 청크 사이즈의 2배 보다 조금 적게 빠르다.
우리는 이것이 whisper 모델의 기대 성능에 의한 것이라고 가정한다.
두번째 가능한 이유는 ESIC의 정답 타임스탬프의 불확실성이다.
타임스탬프는 자동 강제 정렬(forced alignment)에 의해 계산 되며, 그러므로 이것들은 아마 비표준 상황에서 덜 정확할 것이다.
예를 들면, 겹치거나 자막이 없는 발화, 머뭇거리거나 중간에 외국어가 삽인되어 있는 경우가 있다.

## 6. System Demonstration

**Demonstration video**
[https://vimeo.com/840442741](https://vimeo.com/840442741)에서 볼 수 있다.
이 영상은 실시간 Whisper-Streaming 출력 결과를 녹화한 영상으로, 하나의 ESIC 문서의 영어와 독일어, 체코어의 원본과 동시 통역을 세개의 병렬 인스턴스를 실시간으로 ASR을 연산하였다.
비디오는 정답 전사의 타이밍과 함께 대조하여 보여준다.
이를 통해 지연시간을 관찰할 수 있다.
비디오는 ASR 에러에 대해 색상으로 강조해두었다.

**Integration with ELITR**
실용적인 사용가능성을 증명하기 위해, 우리는 [실시간 발화 자막과 번역을 다중 입력 및 출력](https://aclanthology.org/2021.eacl-demos.32.pdf)하기 위한 복잡한 분산 시스템을 위해 Whisper-Streaming과 [ELITR(European Live Translator)](https://aclanthology.org/2020.eamt-1.53.pdf) 프레임워크를 통합했다.
Whisper-Streaming 안에서, 우리는 [중계 서버](https://aclanthology.org/2020.iwltp-1.7.pdf)를 구현하고 릴리즈 했다.
클라이언트는 중계서버에 서비스를 요청할 수 있다.
클라이언트는 서버로부터 받은 출력을 추가적으로 처리할 수 있다.
예를 들어, 번역하거나, 웹 서버에 실시간 자막 형태로 다국어 이벤트 참가자들에게 제공할 수 있따.

**Evaluation event**
다국어 컨퍼런스에서 실험적인 실시간 발화 번역 서비스의 한 부분으로 Whisper-Streaming을 평가했다.
이를 위해, 우리는 다섯개의 병렬 whisper-streaming 워커들을 사용하는 파이프라인을 사용했고, 그 중 셋은 오직 ASR(영어, 체코어, 우크라이나어)을 위해 사용했고, 그리고 두개는 번역에 사용했다(체코어 -> 영어 & 우크라이나어 -> 영어)
회의에서 체코어, 영어 그리고 우크라이나어 세개의 언어 스트림이 병렬로 존재했다.
언어들 중 하나는 본 회의장에서 쓰였고, 나머지는 동시 통역사에 의해 제공되었다.

운영자는 언어지식을 사용하여 기술적인 환경과 결과를 조절했고, 필요하면, 스트림을 리다이렉트 하는 옵션을 가지고있다.
그 사건의 정성적인 평가는 Whisper-Streaming이 견고하고 신뢰할 수 있는 요소로 작동하였으며, 영어, 체코어 그리고 우크라이나어의 장문 발화에서 허용할만한 지연시간과 예상보다 높은 품질과 가깝게 서비스를 달성함을 보여준다.

**Demonstration at AACL**
우리 시스템은 IJCNLP-AACL 2023 학회에서 ELITR 프레임워크를 활용하여 시연될 예정이다.
시연에서는 녹음된 음성으로 음성 소스를 시뮬레이션하거나, 참가자가 Whisper가 지원하는 97개 언어 중 어떤 언어로든 마이크에 직접 발화할 수 있도록 하고, 그에 대한 실시간 출력 결과를 관찰할 수 있게 할 예정이다.

## 7. Conclusion

우리는 Whisper-Streaming을 구현하고, 평가하며, 시연하였다.
이는 원래 오프라인 ASR 모델인 Whisper를 효과적으로 실시간 환경에서 작동하도록 만든 도구로, 영어 ESIC 코퍼스 기준 평균 3.3초의 계산 지연을 고려한 지연 시간을 달성하였다.
우리는 이 구현과 그 핵심 구성 요소를, 특히 스트리밍을 위한 LocalAgreement 알고리즘을 상세히 설명하였다.
마지막으로 Whisper-Streaming은 실제 다국어 회의 현장에서의 시연을 통해 높은 견고성과 실용성을 입증하였다.

## 8. Limitations

ESIC corpus에서 수집된 데이터는 비교적 옛날에 만들어졌다.
이로 인해 해당 데이터가 Whisper의 학습 데이터에 포함되었을 가능성이 있으며, 이는 본 연구의 평가 결과에 영향을 줄 수 있다는 우려를 낳는다.
또한, 보다 저렴한 하드웨어 환경에서의 성능 테스트는 아직 수행되지 않았으며, 이는 계산 비용 측면에서 추가적인 평가의 필요성을 시사한다.

ESIC 코퍼스를 기반으로 측정된 지연 시간 및 품질 지표는, 코퍼스의 특성상 다른 언어나 언어 변종에 일반화되기 어려울 수 있음을 주의해야 한다.

또한, 본 연구의 초점은 Whisper의 온라인(실시간) 처리 가능성을 시연하는 데 있으며, 알고리즘이나 구현을 최적화하는 데에 있지 않다. 실제로 사용자 환경에서 경험하는 지연 시간은 변동될 수 있으며, 본 논문에서 제시한 평균 지연 시간은 상한을 보장하지 않는 참고 수치에 불과하다.
최대 지연 시간을 보장하려면 스트리밍 정책에 일부 수정이 필요하며, 그 경우 전사 품질이 일부 저하될 수 있다.

마지막으로, 우리는 IWSLT와 같은 최신 시스템들과의 비교 실험은 수행하지 않았다. 이는 아직 공통 평가 프레임워크와 X-영어 장문 음성 테스트 세트가 준비되지 않았기 때문이다.
